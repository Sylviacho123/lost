# -*- coding: utf-8 -*-
"""0705

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nU6K6wqE-SoyQYqdwLhn3PkyE0lxpGuC

<a href="https://colab.research.google.com/github/yenlung/Python-AI-Book/blob/main/%E5%86%92%E9%9A%AA11_12_DNN_%E5%81%9A%E6%89%8B%E5%AF%AB%E8%BE%A8%E8%AD%98.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import tensorflow as tf
import numpy as np
from PIL import Image
import gradio as gr

# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
model = tf.keras.models.load_model("lost_item_classifier.keras")

# é¡åˆ¥æ¨™ç±¤ï¼ˆæ ¹æ“šä½ è¨“ç·´æ™‚çš„é †åºï¼‰
labels = ['phone', 'wallet', 'key', 'umbrella', 'backpack']

def recognize_object(image):
    # åœ–ç‰‡è™•ç†
    image = image.convert("RGB").resize((224, 224))
    img_array = np.array(image).astype("float32") / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # é æ¸¬
    predictions = model.predict(img_array)[0]
    predicted_index = np.argmax(predictions)
    predicted_label = labels[predicted_index]

    return f"é€™æ˜¯ï¼š{predicted_label}"

# å»ºç«‹ Gradio ä»‹é¢
iface = gr.Interface(
    fn=recognize_object,
    inputs=gr.Image(type="pil", label="ä¸Šå‚³éºå¤±ç‰©åœ–ç‰‡"),
    outputs=gr.Textbox(label="é æ¸¬çµæœ"),
    title="ğŸ“¦ éºå¤±ç‰©è¾¨è­˜ç³»çµ±",
    description="è«‹ä¸Šå‚³æ‰‹æ©Ÿã€éŒ¢åŒ…ã€é‘°åŒ™ã€é›¨å‚˜æˆ–æ›¸åŒ…çš„åœ–ç‰‡ï¼Œæˆ‘æœƒå¹«ä½ åˆ¤æ–·æ˜¯ä»€éº¼ã€‚"
)

iface.launch()

# âœ… å®‰è£å¿…è¦å¥—ä»¶ï¼ˆåªéœ€è·‘ä¸€æ¬¡ï¼‰
!pip install icrawler tensorflow pillow matplotlib --quiet

import os
from icrawler.builtin import GoogleImageCrawler

# ğŸ“ åˆ†é¡è¨­å®š
categories = ['phone', 'wallet', 'key', 'umbrella', 'backpack']
num_images = 50
data_dir = 'dataset'

# ğŸ“¥ è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾èˆ‡æŠ“åœ–
for category in categories:
    save_dir = os.path.join(data_dir, category)
    os.makedirs(save_dir, exist_ok=True)
    crawler = GoogleImageCrawler(storage={'root_dir': save_dir})
    crawler.crawl(keyword=category, max_num=num_images)

print("âœ… åœ–ç‰‡ä¸‹è¼‰å®Œæˆï¼")


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# åƒæ•¸è¨­å®š
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 10
NUM_CLASSES = len(categories)

# åœ–ç‰‡è³‡æ–™è™•ç†èˆ‡è³‡æ–™å¢å¼·
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=15,
    zoom_range=0.1,
    horizontal_flip=True
)

train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

print("ğŸ“š é¡åˆ¥å°æ‡‰ï¼š", train_generator.class_indices)

# CNN æ¨¡å‹æ¶æ§‹
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ğŸ‹ï¸ é–‹å§‹è¨“ç·´
history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)

# ğŸ’¾ å„²å­˜æ¨¡å‹
model.save("lost_item_classifier.keras")
print("âœ… æ¨¡å‹å·²å„²å­˜ç‚º 'lost_item_classifier.keras'")


import numpy as np
from PIL import Image
import gradio as gr
import tensorflow as tf

# è¼‰å…¥æ¨¡å‹
model = tf.keras.models.load_model("lost_item_classifier.keras")
labels = ['phone', 'wallet', 'key', 'umbrella', 'backpack']  # æ ¹æ“šä½ çš„é¡åˆ¥

def recognize_object(image):
    image = image.convert("RGB").resize((224, 224))
    img_array = np.array(image).astype("float32") / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    predictions = model.predict(img_array)[0]
    index = np.argmax(predictions)
    confidence = predictions[index]

    return f"é€™æ˜¯ï¼š{labels[index]}ï¼ˆä¿¡å¿ƒï¼š{confidence:.2%}ï¼‰"

# Gradio ä»‹é¢
iface = gr.Interface(
    fn=recognize_object,
    inputs=gr.Image(type="pil", label="ä¸Šå‚³éºå¤±ç‰©åœ–ç‰‡"),
    outputs=gr.Textbox(label="é æ¸¬çµæœ"),
    title="ğŸ“¦ éºå¤±ç‰©è¾¨è­˜ç³»çµ±",
    description="è«‹ä¸Šå‚³æ‰‹æ©Ÿã€éŒ¢åŒ…ã€é‘°åŒ™ã€é›¨å‚˜æˆ–æ›¸åŒ…çš„åœ–ç‰‡ï¼Œæˆ‘æœƒå¹«ä½ åˆ¤æ–·æ˜¯ä»€éº¼ã€‚"
)

iface.launch()


iface.launch(share=True)